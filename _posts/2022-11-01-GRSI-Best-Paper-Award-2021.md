---
layout: post
title:  "GRSI Best Paper Award 2021"
date:   2021-11-01 19:41:59 +0000
image: ./assets/award.jpg
category: award
description: Recognition to authors for making their data and code open source!
author: Rozenn Dahyot
---

As reported in <a href="https://doi.org/10.1016/j.cag.2022.11.004" target="_blank">10.1016/j.cag.2022.11.004</a>
our paper on <a href="https://doi.org/10.1016/j.cag.2021.07.022" target="_blank">Model for predicting perception of facial action unit 
activation using virtual humans</a> published in Computers and Graphics Journal 
won the Graphics Replicability Stamp Initiative (GRSI) Best Paper Award!

The <a href="https://www.replicabilitystamp.org/">Graphics Replicability Stamp Initiative</a> is 
an additional recognition for authors who  in addition to publishing the paper,
provide a complete open-source implementation.

The Award committee  <a href="https://doi.org/10.1016/j.cag.2022.11.004" target="_blank">noted</a>:

>**Model for predicting perception of facial action unit activation using virtual humans**  *makes an important contribution to the perception of human faces and facial expressions in the context of digital humans. Blendshape facial rigs are used extensively in games for creating facial animations of virtual humans. Storing and manipulating large sets of facial meshes (blendshapes) is costly in terms of memory and computation for gaming applications. Blendshape rigs include a set of semantically meaningful expressions that encode how expressive the face of a virtual human will be. The key contribution of this work is the development of models for predicting the perceptual importance of blendshapes, and this is cross-validated through successful prediction from unseen data. This work provides a strong foundation for the development of a universal perceptual error metric suitable for facial rigs of human faces. The authorsâ€™ GitHub repository includes data and models in R-code, allowing others to also investigate a broad range of faces, viewpoints, and facial expressions. This work is important for reducing the computational cost and memory demands for game developers in the creation of expressive digital human characters.*




Co-authors Rachel McDonnell (Trinity College Dublin), Katja Zibrek (Inria), Emma Carrigan (Trinity College Dublin)
and Rozenn Dahyot (Maynooth University) have made their data and code from their study  available on
<a href="https://roznn.github.io/facial-blendshapes/" target="_blank">Github</a>.
